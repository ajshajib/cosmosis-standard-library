; This parameter is used several times in this file, so is
; put in the DEFAULT section and is referenced below as %(2PT_FILE)s
[DEFAULT]
2PT_FILE = likelihood/des-y3/2pt_NG_final_2ptunblind_02_24_21_wnz_covupdate.v2.fits
planck_path = likelihood/planck2018/baseline/plc_3.0

[maxlike]
; we save the best parameters as a new values.ini file so you can start
; future runs from there
output_ini = output/w_maxlike.ini
; A covariance can only be output by some 
; optimization methods (those that find an approximation to it numerically)
output_covmat = output/w_covariance.txt
tolerance = 1e-2

max_posterior = T

; The BFGS method seems to find it a bit harder to actually locate
; the peak, but once it's there it provides you with covariance
; matrix estimate
; method = Nelder-Mead
method = BFGS

; Any minimizer available in scipy can be specified here - they are:
; Nelder-Mead
; Powell
; CG
; BFGS
; Newton-CG
; L-BFGS-B
; TNC
; COBYLA
; SLSQP
; dogleg
; trust-ncg


; we could change sampler = maxlike to sampler=emcee at the start
; to use this instead.
[emcee]
; The emcee sampler uses the concept of walkers, a collection
; of live points.  Sampling is done along lines that connect
; pairs of walkers.  The number of walkers must be at least
; 2*nparam + 1, but in general more than that usually works
; better.
walkers = 96
; This many samples is overkill, just to make the plots
; look a lot nicer
samples = 2000
; This is the interval at which convergence diagnostics
; are performed
nsteps = 5

[nautilus]
n_live = 2000
resume = True
verbose = False

[polychord]
base_dir = output/all-polychord-checkpoints
polychord_outfile_root = all
resume = F
feedback = 3
fast_fraction = 0

;Minimum settings
live_points = 96
num_repeats = 30
tolerance = 0.1

;Settings for paper runs
; live_points = 500
; num_repeats=60
; tolerance=0.01
; boost_posteriors=10.0

[w_phi_cdm_model]
file = background/w_phi_cdm_model/w_phi_cdm_model.py
zmax = 3.0
nz = 301

[planck_py]
file = likelihood/planck_py/planck_py_interface.py
use_low_ell_bins = T
spectra = TTTEEE
year = 2018

[planck_lite]
;Planck 2018 high ell TT,TE and EE + low ell TT + low ell EE (in Planck notations = TT+lowE)
;without CMB lensing
file = likelihood/planck2018/planck_interface.so
;high ell TT,TE and EE lite
data_1 = %(planck_path)s/hi_l/plik_lite/plik_lite_v22_TTTEEE.clik
;low ell TT
data_2 = %(planck_path)s/low_l/commander/commander_dx12_v3_2_29.clik
;low ell EE 
data_3 = %(planck_path)s/low_l/simall/simall_100x143_offlike5_EE_Aplanck_B.clik

;;CMB;;
[planck]
;Planck 2018 high ell TT,TE and EE + low ell TT + low ell EE (in Planck notations = TT+lowE)
;without CMB lensing
file = likelihood/planck2018/planck_interface.so
;high ell TT,TE and EE
data_1 = %(planck_path)s/hi_l/plik/plik_rd12_HM_v22b_TTTEEE.clik
;low ell TT
data_2 = %(planck_path)s/low_l/commander/commander_dx12_v3_2_29.clik
;low ell EE 
data_3 = %(planck_path)s/low_l/simall/simall_100x143_offlike5_EE_Aplanck_B.clik

[act_dr6_lens]
file = ./likelihood/act-dr6-lens/act_dr6_lenslike_interface.py
variant = actplanck_baseline ; actplanck_extended ; actplanck_baseline
lens_only = False ; False when combining with any primary CMB measurement, then a covariance matrix will be used which has been CMB marginalized
like_corrections = True # should be False if lens_only is True

[des_sn5yr]
file = likelihood/des-sn5yr/des_sn5yr.py
; likelihood_only = T

[sample_rdh]
file = utility/rescale_distances_rdh/rescale_distances_rdh.py

[desi]
file = likelihood/bao/desi1-dr1/desi1_dr1.py
desi_data_sets = BGS,LRG1,LRG2,LRG3+ELG1,ELG2,QSO,Lya QSO

[desi_sdss]
file = likelihood/bao/desi_sdss/desi_sdss.py
desi_data_sets = BOSS_MGS,BOSS_galaxy1,BOSS_galaxy2,LRG2,LRG3+ELG1,ELG2,QSO,Lya QSO combined

[pantheon_plus]
file = likelihood/pantheon_plus/pantheon_plus_shoes.py
likelihood_only = T
include_shoes = F

; The Riess 11 likelihood anchors H0 for us
[riess21]
file = likelihood/riess21/riess21.py

[tdcosmo]
; This is the likelihood from https://arxiv.org/abs/2007.02941 (Fig. 7, purple contours).
file = likelihood/tdcosmo/tdcosmo_likelihood.py
distances_computation_module = "camb"
num_distribution_draws = 200
data_sets = 'tdcosmo7+SLACS_IFU'

; The consistency module translates between our chosen parameterization
; and any other that modules in the pipeline may want (e.g. camb)
[consistency]
file = utility/consistency/consistency_interface.py

[bbn_consistency]
file = utility/bbn_consistency/bbn_consistency.py

[fits_nz]
file = number_density/load_nz_fits/load_nz_fits.py
nz_file = %(2PT_FILE)s
data_sets = lens source
prefix_section = T
prefix_extension = T

[fits_nz_lens]
file = number_density/load_nz_fits/load_nz_fits.py
nz_file = %(2PT_FILE)s
data_sets = lens
prefix_section = T
prefix_extension = T

[lens_photoz_width]
file = number_density/photoz_width/photoz_width.py
mode = stretch
sample = nz_lens
bias_section = lens_photoz_errors
interpolation = linear

[lens_photoz_bias]
file = number_density/photoz_bias/photoz_bias.py
mode = additive
sample = nz_lens
bias_section = lens_photoz_errors
interpolation = linear

; hyperrank and source_photoz_bias are exclusive
[hyperrank]
file = number_density/nz_multirank/nz_multirank.py
nz_file = %(2PT_FILE)s
data_set = source
dimensions = 3
bin_ranks= 1 2 4 

[source_photoz_bias]
file = number_density/photoz_bias/photoz_bias.py
mode = additive
sample = nz_source
bias_section = wl_photoz_errors
interpolation = linear

[fast_pt]
file = structure/fast_pt/fast_pt_interface.py
do_ia = T
k_res_fac = 0.5
verbose = F

[IA]
file = intrinsic_alignments/tatt/tatt_interface.py
sub_lowk = F
do_galaxy_intrinsic = F
ia_model = tatt

[pk_to_cl_gg]
file = structure/projection/project_2d.py
lingal-lingal = lens-lens
do_exact = lingal-lingal
do_rsd = True
ell_min_linspaced = 1
ell_max_linspaced = 4
n_ell_linspaced = 5
ell_min_logspaced = 5.
ell_max_logspaced = 5.e5
n_ell_logspaced = 80
limber_ell_start = 200
ell_max_logspaced=1.e5
auto_only=lingal-lingal
sig_over_dchi_exact = 3.5

[pk_to_cl]
file = structure/projection/project_2d.py
ell_min_logspaced = 0.1
ell_max_logspaced = 5.0e5
n_ell_logspaced = 100 
shear-shear = source-source
shear-intrinsic = source-source
intrinsic-intrinsic = source-source
intrinsicb-intrinsicb=source-source
lingal-shear = lens-source
lingal-intrinsic = lens-source
lingal-magnification = lens-lens
magnification-shear = lens-source
magnification-magnification = lens-lens
magnification-intrinsic = lens-source 
verbose = F
get_kernel_peaks = F
sig_over_dchi = 20. 
shear_kernel_dchi = 10. 

[add_magnification]
file = structure/magnification/add_magnification.py
include_intrinsic=T

[add_intrinsic]
file=shear/add_intrinsic/add_intrinsic.py
shear-shear=T
position-shear=T
perbin=F

[add_eb]
file = intrinsic_alignments/add_b_mode/add_b_mode_cl.py

[2pt_shear]
file = shear/cl_to_xi_fullsky/cl_to_xi_interface.py
ell_max = 40000
xi_type = EB
theta_file=%(2PT_FILE)s
bin_avg = T
; these get
input_section_name = shear_cl  shear_cl_bb
output_section_name = shear_xi_plus  shear_xi_minus

[2pt_gal]
file = shear/cl_to_xi_fullsky/cl_to_xi_interface.py
ell_max = 40000
xi_type='00'
theta_file=%(2PT_FILE)s
bin_avg = T

[2pt_gal_shear]
file = shear/cl_to_xi_fullsky/cl_to_xi_interface.py
ell_max = 40000
xi_type='02'
theta_file=%(2PT_FILE)s
bin_avg = T

[shear_m_bias]
file = shear/shear_bias/shear_m_bias.py
m_per_bin = True
; Despite the parameter name, this can operate on xi as well as C_ell.
cl_section = shear_xi_plus shear_xi_minus
cross_section = galaxy_shear_xi
verbose = F

[add_point_mass]
file=shear/point_mass/add_gammat_point_mass.py
add_togammat = False
use_fiducial = True
sigcrit_inv_section = sigma_crit_inv_lens_source

[2pt_like]
file = likelihood/2pt/2pt_point_mass/2pt_point_mass.py
do_pm_marg = True
do_pm_sigcritinv = True
sigma_a = 10000.0
no_det_fac = False
include_norm = True
data_file = %(2PT_FILE)s
data_sets = xip xim gammat wtheta
make_covariance=F
covmat_name=COVMAT

; we put these in a separate file because they are long
%include inis/des_y3_scale_cuts.ini


; The small-scale galaxy galaxy-lensing correlations have uncertain
; enough modelling that we can't use them directly, but the ratio between
; sets of correlations can be used, since it only depends on geometry
[shear_ratio_like]
file = likelihood/des-y3/shear_ratio/shear_ratio_likelihood.py
data_file = likelihood/des-y3/shear_ratio/2pt_NG_final_2ptunblind_02_24_21_wnz_covupdate_sr.pkl
theta_min_1 = 9.0  6.0  4.5  2.5  2.5
theta_min_2 = 9.0  6.0  4.5  2.5  2.5
theta_min_3 = 2.5  2.5  4.5  2.5  2.5 
theta_max = 26.83313651 17.63634989 13.61215672 11.32891161 10.01217238
include_norm = T


[bbn]
file = likelihood/bbn/bbn_ombh2_pitrou20_cooke18/bbn_ombh2_pitrou20_cooke18.py 


; This can be used to simulate a data vector from a pipeline
[save_2pt]
file = likelihood/2pt/save_2pt.py
theta_min = 2.5
theta_max = 250.0
n_theta = 20
real_space = T
make_covariance = F
shear_nz_name = nz_source
position_nz_name = nz_lens
filename = output/sim_y3.fits
overwrite = T
auto_only = galaxy_xi
;cut_wtheta = 1,2 1,3 2,3 1,4 2,4 3,4 1,5 2,5 3,5 4,5
spectrum_sections = shear_xi_plus shear_xi_minus galaxy_shear_xi galaxy_xi
output_extensions = xip xim gammat wtheta
two_thirds_midpoint = T
copy_covariance=data_vectors/${DATAFILE}

