[runtime]
; The emcee sampler, which uses the Goodman & Weare algorithm
sampler = emcee
root = ${PWD}
verbosity = quiet

[pipeline]
; We use two likelihoods, Pantheon (for high redshift) and
; Riess 2011 to anchor H0, which is otherwise degenerate
; with the nuisance parameter M
modules = consistency camb srd_sn
values = forecast_inis/srd_sn_values.ini
extra_output =
debug=F
timing=F

[output]
filename = output/srd_sn_cpl.txt
format = text

[maxlike]
; we save the best parameters as a new values.ini file so you can start
; future runs from there
output_ini = output/w_maxlike.ini
; A covariance can only be output by some 
; optimization methods (those that find an approximation to it numerically)
output_covmat = output/w_covariance.txt
tolerance = 1e-2

max_posterior = T

; The BFGS method seems to find it a bit harder to actually locate
; the peak, but once it's there it provides you with covariance
; matrix estimate
; method = Nelder-Mead
method = BFGS

; Any minimizer available in scipy can be specified here - they are:
; Nelder-Mead
; Powell
; CG
; BFGS
; Newton-CG
; L-BFGS-B
; TNC
; COBYLA
; SLSQP
; dogleg
; trust-ncg


; we could change sampler = maxlike to sampler=emcee at the start
; to use this instead.
[emcee]
; The emcee sampler uses the concept of walkers, a collection
; of live points.  Sampling is done along lines that connect
; pairs of walkers.  The number of walkers must be at least
; 2*nparam + 1, but in general more than that usually works
; better.
walkers = 96
; This many samples is overkill, just to make the plots
; look a lot nicer
samples = 2000
; This is the interval at which convergence diagnostics
; are performed
nsteps = 50

[polychord]
base_dir = output/des_sn5yr-polychord-checkpoints
polychord_outfile_root = des_sn5yr
resume = F
feedback = 3
fast_fraction = 0.1

;Minimum settings
live_points = 250
num_repeats = 30
tolerance = 0.1

;Settings for paper runs
; live_points = 500
; num_repeats=60
; tolerance=0.01
; boost_posteriors=10.0

[w_phi_cdm_model]
file = background/w_phi_cdm_model/w_phi_cdm_model.py
zmax = 3.0
nz = 301

[planck]
file = likelihood/planck_py/planck_py_interface.py
use_low_ell_bins = T
spectra = TTTEEE
year = 2018

[camb]
; For background-only data we do not need a full
; Boltzmann evaluation, just D(z), etc.
; Setting mode=background means we get this.
file = boltzmann/camb/camb_interface.py
mode = background
feedback = 0
use_tabulated_w = F
use_ppf_w = T

; from planck_lite.ini
; mode = cmb
; lmax = 2800          ;max ell to use for cmb calculation
; feedback=2         ;amount of output to print
; AccuracyBoost=1.1 ;CAMB accuracy boost parameter
; do_tensors = True   ;include tensor modes
; do_lensing = True    ;lensing is required w/ Planck data
; NonLinear = lens
; accurate_massive_neutrino_transfers = F
; halofit_version = takahashi

; We need quite fine redshift spacing, because the supernovae
; go down to low z where things are pretty sensitive
nz_background = 300
zmin_background = 0.0
zmax_background = 3.0

[srd_sn]
file = likelihood/srd_sn/srd_sn.py
; likelihood_only = T

[sample_rdh]
file = utility/rescale_distances_rdh/rescale_distances_rdh.py

[desi]
file = likelihood/bao/desi1-dr1-arxiv/desi1_dr1_arxiv.py
desi_data_sets = BGS,LRG1

[pantheon_plus]
file = likelihood/pantheon_plus/pantheon_plus_shoes.py
likelihood_only = T
include_shoes = F

; The Riess 11 likelihood anchors H0 for us
[riess21]
file = likelihood/riess21/riess21.py

; The consistency module translates between our chosen parameterization
; and any other that modules in the pipeline may want (e.g. camb)
[consistency]
file = utility/consistency/consistency_interface.py
